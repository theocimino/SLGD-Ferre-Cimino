
\begin{abstract}
Ce document présente différentes méthodes de résolution numérique des systèmes linéaires à grandes dimensions. Ces systèmes sont fréquemment rencontrés dans de nombreuses applications en sciences et ingénierie. Nous discuterons des problèmes associés à leur résolution, des méthodes itératives populaires comme celles de Jacobi, Gauss-Seidel (GS), Successive Over-Relaxation (SOR) et Symmetric Successive Over-Relaxation (SSOR), avant de comparer leur efficacité pour des matrices spécifiques telles que les matrices tridiagonales et de Poisson.\newline
\end{abstract}


\section{Introduction}
La résolution de systèmes linéaires à grandes dimensions est un problème fondamental en mathématiques appliquées aux sciences et à l'ingénierie. La forme générale d'un tel système est \( A \mathbf{x} = \mathbf{b} \), où \( A \) est une matrice carrée de taille \( n \times n \), \( \mathbf{x} \) est le vecteur inconnu de taille \( n \), et \( \mathbf{b} \) est le vecteur des termes constants. Les méthodes directes, comme la méthode de Gauss, peuvent être coûteuses en termes de mémoire et de temps de calcul lorsque \( n \) est très grand. C'est pourquoi des méthodes itératives sont souvent privilégiées pour de tels systèmes. Ces méthodes, qui génèrent une suite de vecteurs approchants pour la solution, sont particulièrement utiles lorsque la matrice \( A \) possède des propriétés spécifiques (comme les matrices symétriques définies positives par exemple (SDP)).

En particulier, les méthodes de Jacobi, GS, SOR et de SSOR sont largement utilisées. Ces méthodes sont adaptées aux systèmes linéaires avec des matrices creuses, comme les matrices tridiagonales et de Poisson, qui apparaissent fréquemment dans des applications en physique et en ingénierie.

\newline

\section{Méthodes Itératives}
Les méthodes itératives permettent d'approximer la solution d'un système linéaire de manière progressive. Les techniques suivantes sont parmi les plus utilisées dans la résolution de systèmes à grandes dimensions.

\subsection{Méthode de Jacobi}
La méthode de Jacobi est une méthode itérative simple qui consiste à isoler chaque variable dans l'équation \( A \mathbf{x} = \mathbf{b} \) pour exprimer \( x_i \) en fonction des autres variables \( x_j \) :
\[
x_i^{(k+1)} = \frac{b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}}{a_{ii}}.
\]
Ici, \( k \) représente le numéro de l'itération. La méthode de Jacobi est simple à mettre en œuvre, mais elle peut être lente à converger, notamment pour des systèmes de grande taille.

\subsection{Méthode de Gauss-Seidel (GS)}
La méthode de Gauss-Seidel améliore la méthode de Jacobi en utilisant les nouvelles valeurs de \( x_i \) dès qu'elles sont disponibles. L'itération de Gauss-Seidel est donnée par :
\[
x_i^{(k+1)} = \frac{b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)}}{a_{ii}}.
\]
L'avantage principal de la méthode de Gauss-Seidel par rapport à Jacobi est qu'elle converge généralement plus rapidement, bien qu'elle soit également conditionnée par la structure de la matrice \( A \).

\subsection{Méthode de Successive Over-Relaxation (SOR)}
La méthode SOR est une généralisation de la méthode de Gauss-Seidel, dans laquelle un facteur de relaxation \( \omega \) est introduit pour accélérer la convergence. L'itération de SOR est donnée par :
\[
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right).
\]
Le paramètre \( \omega \) est choisi de manière à optimiser la vitesse de convergence de l'itération. Si \( \omega = 1 \), la méthode correspond à la méthode de Gauss-Seidel. Un choix approprié de \( \omega \) peut considérablement améliorer la vitesse de convergence.

\subsection{Méthode SSOR (Symmetric Successive Over-Relaxation)}
La méthode SSOR combine deux passes de SOR : une première passe de SOR pour résoudre un système \( A \mathbf{x} = \mathbf{b} \) et une deuxième passe de SOR pour résoudre le système symétrique associé. Cela peut améliorer la convergence, en particulier pour les systèmes avec une matrice \( A \) symétrique ou de Poisson. L'itération SSOR est donc une combinaison des étapes de relaxation successives avant et après.

\newpage

\section{Comparaison des Méthodes}
Nous comparons maintenant les performances des méthodes de Jacobi, Gauss-Seidel, SOR et SSOR pour des matrices tridiagonales et de Poisson. Pour chaque méthode, nous analysons le nombre d'itérations nécessaires pour atteindre une tolérance donnée et le temps de calcul.

\subsection{Matrices Tridiagonales}
Les matrices tridiagonales sont des matrices carrées où les éléments non nuls se trouvent sur la diagonale principale et les deux diagonales adjacentes. Un exemple de matrice tridiagonale est :
\[
A = \begin{bmatrix}
d_1 & c_1 & 0 & \dots & 0 \\
a_2 & d_2 & c_2 & \dots & 0 \\
0 & a_3 & d_3 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & d_n
\end{bmatrix}.
\]
Les performances des méthodes itératives dépendent fortement de la structure de cette matrice. En général, les méthodes Gauss-Seidel et SOR convergent plus rapidement que Jacobi pour les matrices tridiagonales.

\subsection{Matrices de Poisson}
Les matrices de Poisson sont souvent utilisées dans la résolution de problèmes aux dérivées partielles, comme les équations de la chaleur et de la diffusion. Ces matrices sont de type tridiagonal par blocs, et leur résolution par méthodes itératives pose des défis supplémentaires. Les méthodes SOR et SSOR sont particulièrement efficaces pour les systèmes associés à des matrices de Poisson en raison de leur structure particulière.
